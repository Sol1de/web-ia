{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:43.311656Z",
     "start_time": "2025-12-11T13:17:43.285483600Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "data_loading",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:43.363690200Z",
     "start_time": "2025-12-11T13:17:43.312656400Z"
    }
   },
   "source": [
    "# Normalisation des données\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "dataloaders",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:43.373026200Z",
     "start_time": "2025-12-11T13:17:43.364691900Z"
    }
   },
   "source": [
    "training_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "model_definition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:43.381983800Z",
     "start_time": "2025-12-11T13:17:43.374026900Z"
    }
   },
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # CNN filtres convolutionnels\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # 1x28x28 -> 32x28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 32x28x28 -> 32x14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 32x14x14 -> 64x14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 64x14x14 -> 64x7x7\n",
    "        )\n",
    "        # MLP\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "model_init",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:43.595929600Z",
     "start_time": "2025-12-11T13:17:43.382981900Z"
    }
   },
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "loss_optimizer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:47.914621500Z",
     "start_time": "2025-12-11T13:17:43.596931Z"
    }
   },
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = \"runs/mnist_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "dataiter = iter(training_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "writer.add_images('mnist_examples', images[:16], 0)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs: runs/mnist_20251211-141743\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "train_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:47.995002100Z",
     "start_time": "2025-12-11T13:17:47.985242300Z"
    }
   },
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, batch_value in enumerate(dataloader):\n",
    "        X, y = batch_value\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            loss_val, current = loss.item(), (batch_idx+1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5}]\")\n",
    "            # Log batch loss TensorBoard\n",
    "            global_step = epoch * num_batches + batch_idx\n",
    "            writer.add_scalar('Loss/train_batch', loss_val, global_step)\n",
    "    \n",
    "    # Log average epoch loss et accuracy\n",
    "    avg_loss = running_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', 100 * accuracy, epoch)\n",
    "    \n",
    "    print(f\"Train Accuracy: {(100*accuracy):>0.1f}%\")\n",
    "    return avg_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "test_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:17:48.003158100Z",
     "start_time": "2025-12-11T13:17:47.996014700Z"
    }
   },
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    \n",
    "    # Log TensorBoard\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', 100 * accuracy, epoch)\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "training_loop",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:19:53.775584500Z",
     "start_time": "2025-12-11T13:17:48.004342400Z"
    }
   },
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer, t, writer)\n",
    "    test(test_dataloader, model, loss_fn, t, writer)\n",
    "    \n",
    "    # Log histogrammes des poids à chaque epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f'Parameters/{name}', param, t)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'Gradients/{name}', param.grad, t)\n",
    "\n",
    "writer.close()\n",
    "print(\"Done!\")\n",
    "print(f\"\\nPour visualiser TensorBoard, executez: .venv\\\\Scripts\\\\tensorboard --logdir=runs\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.467238  [   64/60000]\n",
      "loss: 1.161456  [ 6464/60000]\n",
      "loss: 0.881448  [12864/60000]\n",
      "loss: 0.726967  [19264/60000]\n",
      "loss: 0.748845  [25664/60000]\n",
      "loss: 0.653951  [32064/60000]\n",
      "loss: 0.556463  [38464/60000]\n",
      "loss: 0.615584  [44864/60000]\n",
      "loss: 0.615553  [51264/60000]\n",
      "loss: 0.575344  [57664/60000]\n",
      "Train Accuracy: 84.3%\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.448914 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.540701  [   64/60000]\n",
      "loss: 0.529139  [ 6464/60000]\n",
      "loss: 0.442498  [12864/60000]\n",
      "loss: 0.439419  [19264/60000]\n",
      "loss: 0.399032  [25664/60000]\n",
      "loss: 0.359559  [32064/60000]\n",
      "loss: 0.401901  [38464/60000]\n",
      "loss: 0.276508  [44864/60000]\n",
      "loss: 0.275279  [51264/60000]\n",
      "loss: 0.307015  [57664/60000]\n",
      "Train Accuracy: 93.4%\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.276948 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.350773  [   64/60000]\n",
      "loss: 0.406567  [ 6464/60000]\n",
      "loss: 0.279225  [12864/60000]\n",
      "loss: 0.260993  [19264/60000]\n",
      "loss: 0.301081  [25664/60000]\n",
      "loss: 0.318311  [32064/60000]\n",
      "loss: 0.249118  [38464/60000]\n",
      "loss: 0.273801  [44864/60000]\n",
      "loss: 0.270605  [51264/60000]\n",
      "loss: 0.246192  [57664/60000]\n",
      "Train Accuracy: 95.2%\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.202721 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.249715  [   64/60000]\n",
      "loss: 0.267261  [ 6464/60000]\n",
      "loss: 0.306694  [12864/60000]\n",
      "loss: 0.294424  [19264/60000]\n",
      "loss: 0.197875  [25664/60000]\n",
      "loss: 0.344957  [32064/60000]\n",
      "loss: 0.190817  [38464/60000]\n",
      "loss: 0.236808  [44864/60000]\n",
      "loss: 0.188151  [51264/60000]\n",
      "loss: 0.190348  [57664/60000]\n",
      "Train Accuracy: 96.0%\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.159263 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.187742  [   64/60000]\n",
      "loss: 0.211473  [ 6464/60000]\n",
      "loss: 0.216665  [12864/60000]\n",
      "loss: 0.146035  [19264/60000]\n",
      "loss: 0.187591  [25664/60000]\n",
      "loss: 0.171715  [32064/60000]\n",
      "loss: 0.189339  [38464/60000]\n",
      "loss: 0.158557  [44864/60000]\n",
      "loss: 0.175017  [51264/60000]\n",
      "loss: 0.107575  [57664/60000]\n",
      "Train Accuracy: 96.6%\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.134124 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.173583  [   64/60000]\n",
      "loss: 0.208094  [ 6464/60000]\n",
      "loss: 0.188146  [12864/60000]\n",
      "loss: 0.171609  [19264/60000]\n",
      "loss: 0.172685  [25664/60000]\n",
      "loss: 0.162204  [32064/60000]\n",
      "loss: 0.124811  [38464/60000]\n",
      "loss: 0.160888  [44864/60000]\n",
      "loss: 0.154092  [51264/60000]\n",
      "loss: 0.124195  [57664/60000]\n",
      "Train Accuracy: 97.0%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.113297 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.236761  [   64/60000]\n",
      "loss: 0.132347  [ 6464/60000]\n",
      "loss: 0.208753  [12864/60000]\n",
      "loss: 0.310539  [19264/60000]\n",
      "loss: 0.086472  [25664/60000]\n",
      "loss: 0.138255  [32064/60000]\n",
      "loss: 0.151381  [38464/60000]\n",
      "loss: 0.175905  [44864/60000]\n",
      "loss: 0.074716  [51264/60000]\n",
      "loss: 0.138045  [57664/60000]\n",
      "Train Accuracy: 97.3%\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.103279 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.233095  [   64/60000]\n",
      "loss: 0.127654  [ 6464/60000]\n",
      "loss: 0.093710  [12864/60000]\n",
      "loss: 0.146153  [19264/60000]\n",
      "loss: 0.178244  [25664/60000]\n",
      "loss: 0.085058  [32064/60000]\n",
      "loss: 0.101477  [38464/60000]\n",
      "loss: 0.120714  [44864/60000]\n",
      "loss: 0.223311  [51264/60000]\n",
      "loss: 0.070110  [57664/60000]\n",
      "Train Accuracy: 97.5%\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.094878 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.130202  [   64/60000]\n",
      "loss: 0.154356  [ 6464/60000]\n",
      "loss: 0.146659  [12864/60000]\n",
      "loss: 0.096200  [19264/60000]\n",
      "loss: 0.176249  [25664/60000]\n",
      "loss: 0.097863  [32064/60000]\n",
      "loss: 0.138851  [38464/60000]\n",
      "loss: 0.136864  [44864/60000]\n",
      "loss: 0.241299  [51264/60000]\n",
      "loss: 0.084797  [57664/60000]\n",
      "Train Accuracy: 97.6%\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.084927 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.091773  [   64/60000]\n",
      "loss: 0.110571  [ 6464/60000]\n",
      "loss: 0.095299  [12864/60000]\n",
      "loss: 0.087490  [19264/60000]\n",
      "loss: 0.100964  [25664/60000]\n",
      "loss: 0.074923  [32064/60000]\n",
      "loss: 0.098550  [38464/60000]\n",
      "loss: 0.113932  [44864/60000]\n",
      "loss: 0.190137  [51264/60000]\n",
      "loss: 0.107957  [57664/60000]\n",
      "Train Accuracy: 97.8%\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.078811 \n",
      "\n",
      "Done!\n",
      "\n",
      "Pour visualiser TensorBoard, executez: .venv\\Scripts\\tensorboard --logdir=runs\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "export_onnx",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T13:19:54.631146800Z",
     "start_time": "2025-12-11T13:19:53.904228900Z"
    }
   },
   "source": [
    "example_inputs = (torch.randn(1,1,28,28),)\n",
    "model.to(\"cpu\")\n",
    "onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)\n",
    "onnx_program.save(\"model_rendu.onnx\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 3 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
