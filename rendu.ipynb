{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:43:55.958380300Z",
     "start_time": "2025-12-11T15:43:55.891747500Z"
    }
   },
   "source": "import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor, Compose, Normalize\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nimport datetime\nfrom datasets import load_dataset\nfrom PIL import Image",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "data_loading",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:03.040754Z",
     "start_time": "2025-12-11T15:43:55.972644800Z"
    }
   },
   "source": [
    "# Dataset wrapper HF\n",
    "class HFMNISTDataset(Dataset):\n",
    "    def __init__(self, hf_data, transform=None):\n",
    "        self.data = hf_data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        if img.mode != \"L\":\n",
    "            img = img.convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "print(\"Chargement de MNIST depuis Hugging Face...\")\n",
    "hf_dataset = load_dataset(\"mnist\")\n",
    "print(f\"Dataset chargé: {hf_dataset}\")\n",
    "\n",
    "# Calcul moyenne/variance du dataset\n",
    "print(\"\\nCalcul des statistiques de normalisation...\")\n",
    "temp_transform = ToTensor()\n",
    "sample_size = min(10000, len(hf_dataset[\"train\"]))\n",
    "samples = torch.stack([temp_transform(hf_dataset[\"train\"][i][\"image\"]) for i in range(sample_size)])\n",
    "\n",
    "mean = samples.mean().item()\n",
    "std = samples.std().item()\n",
    "variance = samples.var().item()\n",
    "\n",
    "print(f\"Moyenne: {mean:.6f}\")\n",
    "print(f\"Écart-type: {std:.6f}\")\n",
    "print(f\"Variance: {variance:.6f}\")\n",
    "\n",
    "# Normalisation\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((mean,), (std,))\n",
    "])\n",
    "\n",
    "# Création des datasets PyTorch\n",
    "training_data = HFMNISTDataset(hf_dataset[\"train\"], transform=transform)\n",
    "test_data = HFMNISTDataset(hf_dataset[\"test\"], transform=transform)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(training_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de MNIST depuis Hugging Face...\n",
      "Dataset chargé: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 60000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "\n",
      "Calcul des statistiques de normalisation...\n",
      "Moyenne: 0.131126\n",
      "Écart-type: 0.308699\n",
      "Variance: 0.095295\n",
      "\n",
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "dataloaders",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.524031400Z",
     "start_time": "2025-12-11T15:44:04.494149800Z"
    }
   },
   "source": "# Note: num_workers=0 nécessaire sur Windows avec datasets HF\n# (problème de sérialisation des objets PIL dans les workers)\ntraining_dataloader = DataLoader(\n    training_data,\n    batch_size=64,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=0\n)\n\ntest_dataloader = DataLoader(\n    test_data,\n    batch_size=64,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=0\n)",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "model_definition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.541368700Z",
     "start_time": "2025-12-11T15:44:04.525811800Z"
    }
   },
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # CNN filtres convolutionnels\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # 1x28x28 -> 32x28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 32x28x28 -> 32x14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 32x14x14 -> 64x14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 64x14x14 -> 64x7x7\n",
    "        )\n",
    "        # MLP\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "model_init",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.566618400Z",
     "start_time": "2025-12-11T15:44:04.542596100Z"
    }
   },
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "loss_optimizer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.764763400Z",
     "start_time": "2025-12-11T15:44:04.578250200Z"
    }
   },
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = \"runs/mnist_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "dataiter = iter(training_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "writer.add_images('mnist_examples', images[:16], 0)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs: runs/mnist_20251211-164404\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "train_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.781958800Z",
     "start_time": "2025-12-11T15:44:04.764763400Z"
    }
   },
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, batch_value in enumerate(dataloader):\n",
    "        X, y = batch_value\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            loss_val, current = loss.item(), (batch_idx+1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5}]\")\n",
    "            # Log batch loss TensorBoard\n",
    "            global_step = epoch * num_batches + batch_idx\n",
    "            writer.add_scalar('Loss/train_batch', loss_val, global_step)\n",
    "    \n",
    "    # Log average epoch loss et accuracy\n",
    "    avg_loss = running_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', 100 * accuracy, epoch)\n",
    "    \n",
    "    print(f\"Train Accuracy: {(100*accuracy):>0.1f}%\")\n",
    "    return avg_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "test_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:44:04.792723900Z",
     "start_time": "2025-12-11T15:44:04.783490100Z"
    }
   },
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    \n",
    "    # Log TensorBoard\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', 100 * accuracy, epoch)\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "training_loop",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:50:00.895087800Z",
     "start_time": "2025-12-11T15:44:04.797126500Z"
    }
   },
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer, t, writer)\n",
    "    test(test_dataloader, model, loss_fn, t, writer)\n",
    "    \n",
    "    # Log histogrammes des poids à chaque epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f'Parameters/{name}', param, t)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'Gradients/{name}', param.grad, t)\n",
    "\n",
    "writer.close()\n",
    "print(\"Done!\")\n",
    "print(f\"\\nPour visualiser TensorBoard, executez: .venv\\\\Scripts\\\\tensorboard --logdir=runs\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.349527  [   64/60000]\n",
      "loss: 1.307376  [ 6464/60000]\n",
      "loss: 0.972579  [12864/60000]\n",
      "loss: 0.884647  [19264/60000]\n",
      "loss: 0.659630  [25664/60000]\n",
      "loss: 0.809253  [32064/60000]\n",
      "loss: 0.568069  [38464/60000]\n",
      "loss: 0.560241  [44864/60000]\n",
      "loss: 0.557123  [51264/60000]\n",
      "loss: 0.541933  [57664/60000]\n",
      "Train Accuracy: 85.8%\n",
      "Test Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.425827 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.518464  [   64/60000]\n",
      "loss: 0.581524  [ 6464/60000]\n",
      "loss: 0.440115  [12864/60000]\n",
      "loss: 0.433454  [19264/60000]\n",
      "loss: 0.339873  [25664/60000]\n",
      "loss: 0.374732  [32064/60000]\n",
      "loss: 0.397595  [38464/60000]\n",
      "loss: 0.342497  [44864/60000]\n",
      "loss: 0.367834  [51264/60000]\n",
      "loss: 0.324078  [57664/60000]\n",
      "Train Accuracy: 94.2%\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.265559 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.325193  [   64/60000]\n",
      "loss: 0.437616  [ 6464/60000]\n",
      "loss: 0.250445  [12864/60000]\n",
      "loss: 0.201337  [19264/60000]\n",
      "loss: 0.233734  [25664/60000]\n",
      "loss: 0.225803  [32064/60000]\n",
      "loss: 0.292925  [38464/60000]\n",
      "loss: 0.200540  [44864/60000]\n",
      "loss: 0.208551  [51264/60000]\n",
      "loss: 0.211706  [57664/60000]\n",
      "Train Accuracy: 95.6%\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.193682 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.273130  [   64/60000]\n",
      "loss: 0.223190  [ 6464/60000]\n",
      "loss: 0.281463  [12864/60000]\n",
      "loss: 0.256162  [19264/60000]\n",
      "loss: 0.259513  [25664/60000]\n",
      "loss: 0.179996  [32064/60000]\n",
      "loss: 0.210745  [38464/60000]\n",
      "loss: 0.187145  [44864/60000]\n",
      "loss: 0.196040  [51264/60000]\n",
      "loss: 0.334275  [57664/60000]\n",
      "Train Accuracy: 96.3%\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.156770 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.220549  [   64/60000]\n",
      "loss: 0.227501  [ 6464/60000]\n",
      "loss: 0.186410  [12864/60000]\n",
      "loss: 0.123495  [19264/60000]\n",
      "loss: 0.188557  [25664/60000]\n",
      "loss: 0.122683  [32064/60000]\n",
      "loss: 0.186436  [38464/60000]\n",
      "loss: 0.275992  [44864/60000]\n",
      "loss: 0.134877  [51264/60000]\n",
      "loss: 0.226674  [57664/60000]\n",
      "Train Accuracy: 96.7%\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.132243 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.203022  [   64/60000]\n",
      "loss: 0.156164  [ 6464/60000]\n",
      "loss: 0.227334  [12864/60000]\n",
      "loss: 0.266436  [19264/60000]\n",
      "loss: 0.157143  [25664/60000]\n",
      "loss: 0.195539  [32064/60000]\n",
      "loss: 0.175055  [38464/60000]\n",
      "loss: 0.196979  [44864/60000]\n",
      "loss: 0.185332  [51264/60000]\n",
      "loss: 0.157893  [57664/60000]\n",
      "Train Accuracy: 97.1%\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.114382 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.152749  [   64/60000]\n",
      "loss: 0.243768  [ 6464/60000]\n",
      "loss: 0.128454  [12864/60000]\n",
      "loss: 0.115704  [19264/60000]\n",
      "loss: 0.162701  [25664/60000]\n",
      "loss: 0.128128  [32064/60000]\n",
      "loss: 0.178995  [38464/60000]\n",
      "loss: 0.147852  [44864/60000]\n",
      "loss: 0.086155  [51264/60000]\n",
      "loss: 0.147447  [57664/60000]\n",
      "Train Accuracy: 97.3%\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.102141 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.146508  [   64/60000]\n",
      "loss: 0.095609  [ 6464/60000]\n",
      "loss: 0.079056  [12864/60000]\n",
      "loss: 0.203281  [19264/60000]\n",
      "loss: 0.142170  [25664/60000]\n",
      "loss: 0.165080  [32064/60000]\n",
      "loss: 0.116497  [38464/60000]\n",
      "loss: 0.157124  [44864/60000]\n",
      "loss: 0.114106  [51264/60000]\n",
      "loss: 0.163842  [57664/60000]\n",
      "Train Accuracy: 97.4%\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.092677 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.103224  [   64/60000]\n",
      "loss: 0.162967  [ 6464/60000]\n",
      "loss: 0.111439  [12864/60000]\n",
      "loss: 0.102980  [19264/60000]\n",
      "loss: 0.244338  [25664/60000]\n",
      "loss: 0.093209  [32064/60000]\n",
      "loss: 0.088090  [38464/60000]\n",
      "loss: 0.163956  [44864/60000]\n",
      "loss: 0.128576  [51264/60000]\n",
      "loss: 0.104136  [57664/60000]\n",
      "Train Accuracy: 97.6%\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.086689 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.099911  [   64/60000]\n",
      "loss: 0.137636  [ 6464/60000]\n",
      "loss: 0.100497  [12864/60000]\n",
      "loss: 0.126599  [19264/60000]\n",
      "loss: 0.061025  [25664/60000]\n",
      "loss: 0.113444  [32064/60000]\n",
      "loss: 0.077170  [38464/60000]\n",
      "loss: 0.122298  [44864/60000]\n",
      "loss: 0.051366  [51264/60000]\n",
      "loss: 0.083881  [57664/60000]\n",
      "Train Accuracy: 97.7%\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.080704 \n",
      "\n",
      "Done!\n",
      "\n",
      "Pour visualiser TensorBoard, executez: .venv\\Scripts\\tensorboard --logdir=runs\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "export_onnx",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:50:01.776792400Z",
     "start_time": "2025-12-11T15:50:01.004884300Z"
    }
   },
   "source": [
    "example_inputs = (torch.randn(1,1,28,28),)\n",
    "model.to(\"cpu\")\n",
    "onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)\n",
    "onnx_program.save(\"model_rendu.onnx\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 3 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
